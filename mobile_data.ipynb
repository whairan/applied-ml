{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whairan/applied-ml/blob/main/mobile_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Workshop - I\n",
        "\n",
        "## Objective:\n",
        "In this module, we will explore various regression and classification models to evaluate their performance on a given dataset, ultimately determining which model delivers the best results:\n",
        "\n",
        "- **1. Regression Models:**\n",
        "     - Dataset Description\n",
        "\n",
        "     - Conduct Exploratory Data Analysis (EDA) on the dataset.\n",
        "      - a. Basic Information\n",
        "      - b. Examine the Target Variable\n",
        "      - c. Examine Feature Variables\n",
        "      - d. Feature-Target Relationship\n",
        "      - e. Check for Multicollinearity\n",
        "\n",
        "     - Data pre-processing\n",
        "\n",
        "     - Train and evaluate different regression models:\n",
        "      - a. Simple Linear Regression\n",
        "      - b. Polynomial Regression\n",
        "     \n",
        "\n",
        "- **2. Classification Models:**\n",
        "     - Dataset Description\n",
        "\n",
        "     - Conduct Exploratory Data Analysis (EDA) on the dataset.\n",
        "      - a. Basic Information\n",
        "      - b. Examine the Target Variable\n",
        "      - c. Examine Feature Variables\n",
        "      - d. Check for Multicollinearity\n",
        "\n",
        "     - Data pre-processing\n",
        "\n",
        "     - Train and evaluate different classification models:\n",
        "      - a. Logistic Regression\n",
        "      - b. K-Nearest Neighbours (KNN)\n",
        "      - c. Naive Bayes\n",
        "\n",
        "     - Hyperparameter tuning using techniques such as   GridSearchCV or RandomizedSearchCV"
      ],
      "metadata": {
        "id": "etU94s---Nyc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Regression Models\n",
        "\n"
      ],
      "metadata": {
        "id": "vAJgmxSXBWLl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Dataset Description\n",
        "\n",
        "<center><img src=\"https://i.pcmag.com/imagery/articles/00p0Qzwvctko7LmNqSYnPAE-10..v1713208898.jpg\" width=500/></center>\n",
        "\n",
        "**Problem Statement:**\n",
        "\n",
        "Develop a model that predicts the price range of mobile phones based on their features and specifications. The goal is to classify mobile devices into different price categories (e.g., low, medium, high) using attributes such as battery life, screen size, RAM, processor speed, and camera quality. This model will assist manufacturers, retailers, and consumers in understanding how various features contribute to the overall cost of mobile devices.\n",
        "\n",
        "**Dataset:**\n",
        "Dataset is taken from kaggle - <a href='https://www.kaggle.com/datasets/ganjerlawrence/mobile-phone-price-prediction-cleaned-dataset'>link</a>\n",
        "\n",
        "This data set is a collection for information about various mobile phone type . it is a cleaned data set made available for beginners who want to learn on how to implement machine learning algorithms rather than spending time cleaning the data set. you can just concentrate on building your models\n",
        "\n",
        "The columns available in the dataset are:-\n",
        "\n",
        "- **Ratings:** This field contains the various rating given by customers\n",
        "- **Ram:** This field contain the Ram capacity of the phone in GB\n",
        "- **ROM:** This is field contains the number of space available in the phone in GB\n",
        "- **Mobile_Size:** This is the int size of the screen\n",
        "- **Primary_Cam:** This is the number of pixels of the Back camera\n",
        "- **Selfi_Cam:** The number of Pixels of the front camera\n",
        "- **Battery_Power:** The battery power\n",
        "- **Price:** The price of the mobile phone in INR(indian Rupees)\n",
        "\n",
        "All the fields are already in numerical values either `int` or `float`.\n",
        "\n"
      ],
      "metadata": {
        "id": "_9-JNYnV3rR9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Importing the necessary libraries & modules"
      ],
      "metadata": {
        "id": "OR9wLHftuElT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the warnings module, which allows control over warning messages\n",
        "import warnings\n",
        "\n",
        "# Suppress all warnings in the script\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "W8r7zH2s0ZgF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "49pd2ZT53rRy"
      },
      "outputs": [],
      "source": [
        "# Importing libraries for numerical operations and data manipulation\n",
        "import numpy as np     # For numerical computations\n",
        "import pandas as pd    # For handling dataframes and data manipulation\n",
        "\n",
        "# Importing libraries for data visualization\n",
        "import matplotlib.pyplot as plt  # For plotting graphs\n",
        "import seaborn as sns            # For enhanced visualizations\n",
        "\n",
        "# Importing metrics for evaluating regression model performance\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error  # Metrics to evaluate models\n",
        "\n",
        "# Importing utilities for splitting data and applying feature transformations\n",
        "from sklearn.model_selection import train_test_split  # For splitting data into training and testing sets\n",
        "\n",
        "\n",
        "# Importing models for regression tasks\n",
        "from sklearn.linear_model import LinearRegression     # For linear regression model\n",
        "\n",
        "\n",
        "# Importing libraries for statistical analysis\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor  # To calculate VIF for multicollinearity\n",
        "import statsmodels.api as sm   # For advanced statistical modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Loading the data\n",
        "\n",
        "- `read_csv()` helps to load a csv file.\n",
        "- `head()` helps to view the initial few rows of the dataset."
      ],
      "metadata": {
        "id": "GppmEXv-txPl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset from a CSV file into a DataFrame\n",
        "\n",
        "\n",
        "# Display the first five rows of the DataFrame to get an initial overview of the data\n"
      ],
      "metadata": {
        "id": "kruQHjPn3rR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.4 Exploratory Data Analysis (EDA)"
      ],
      "metadata": {
        "id": "Ul634_lX3rR9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.4.1 Basic Information\n",
        "\n",
        " Get an initial overview of the dataset to understand its structure, types of variables, and any immediate data quality issues.\n",
        "\n",
        "- `info()` gives information about the dataset.\n",
        "- `describe()` provides the basic statistical details about the dataset."
      ],
      "metadata": {
        "id": "DAZgygci3rR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display a summary of the Dataset\n"
      ],
      "metadata": {
        "id": "9Kr8Fesn3rR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpretation:\n",
        "- Do all the columns have the correct data types?\n",
        "  \n",
        "  \n",
        "\n",
        "- Are there any null values in the dataset?\n",
        "  \n",
        "  \n",
        "  "
      ],
      "metadata": {
        "id": "CAworvWhuWWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate descriptive statistics for the numerical columns in the data\n"
      ],
      "metadata": {
        "id": "UZEyg5s-3rR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpretation:\n",
        "- Which features might contain outliers? Why?\n",
        "\n"
      ],
      "metadata": {
        "id": "Li8V1fJVwlVQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4.2 Distribution Analysis of the target variable\n",
        "\n",
        "*Hint: Use the kdeplot*\n",
        "- `sns.kdeplot()` visualizes the distribution of observations in a dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "gyTjx7Ke3rR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KDE plot of the target column\n",
        "# Replace the placeholder <> with the target column of the loaded dataset . Eg. df['Price']\n",
        "sns.kdeplot(<>, shade=True)\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('Target')\n",
        "plt.ylabel('Density')\n",
        "plt.title('KDE Plot of Target Column')\n",
        "\n",
        "# Show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JyiCts003rR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpretation:\n",
        "- What does the KDE plot of the target column indicate about the distribution?\n",
        "\n",
        "\n",
        "\n",
        "- What does the shape of this distribution suggest about the nature of the target variable?\n",
        "\n",
        "  \n",
        "- Are there any outliers or extreme values present in the target column?\n",
        "\n",
        "\n",
        "- Where is the peak of the distribution located?\n",
        "\n",
        "  \n",
        "- How does the density change across the range of target values?\n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "id": "X3ptzZfO3rR-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4.3 Outlier Analysis of the target variable\n",
        "\n",
        "*Hint: Use the boxplot*\n",
        "- `sns.boxplot()` helps to visualize the observations using a boxplot."
      ],
      "metadata": {
        "id": "KoP5dFNL3rR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Outlier analysis using boxplot\n",
        "# Replace the placeholder <> with the boxplot code - `sns.boxplot()` containing the target column `df['Price]` as the parameter\n",
        "<>\n",
        "plt.title('Box Plot of Price Variable')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XKxVTayr3rR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpretation:\n",
        "- Are there any outliers present in the \"Price\" variable?\n",
        "\n",
        "\n",
        "- Where is the median of the `Price` variable located?\n",
        "  \n"
      ],
      "metadata": {
        "id": "fh03knHn3rR-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4.4 Correlation Analysis of the dataset\n",
        "\n",
        "*Hint: Use heatmap*\n",
        "- `corr()` calculates the correlation between columns in a DataFrame.\n",
        "- `sns.heatmap()` helps to visualize the correlation values as a color coded matrix."
      ],
      "metadata": {
        "id": "1MrpjH4c3rR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the correlation matrix for the DataFrame to understand the relationships between numerical variables\n",
        "# Replace the placeholder with the code to find the correlation `df.corr()` and assign it to a variable say `corr`\n",
        "<>\n",
        "\n",
        "\n",
        "# Set up the figure for the heatmap with a size of 10x8 inches\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Plot the heatmap of the correlation matrix\n",
        "# annot=True adds the correlation coefficients on the heatmap\n",
        "# cmap='Blues' specifies the color map\n",
        "# fmt='.2f' formats the annotation to two decimal places\n",
        "\n",
        "# Replace the placeholder <1> with the code for creating the heatmap - `sns.heatmap()`\n",
        "# Replace the placeholder <2> with the variable containing correlation values say 'corr'\n",
        "<1>(<2>, annot=True, cmap='Blues', fmt='.2f')\n",
        "\n",
        "# Add a title to the heatmap\n",
        "plt.title('Correlation Heatmap')\n",
        "\n",
        "# Display the heatmap\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kHpXats23rR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpretation:\n",
        "\n",
        "- Which features have the strongest positive correlation with `Price`?\n",
        "  \n",
        "\n",
        "- Is there any feature with a negative correlation with `Price`?\n",
        "  \n",
        "\n",
        "- How strongly are `RAM` and `Price` correlated?\n",
        "  \n",
        "- What is the relationship between `Ratings` and `Battery_Power`?\n",
        "  \n",
        "\n",
        "- Are there any features that have little to no correlation with `Price`?\n",
        "\n",
        "  \n",
        "\n",
        "- Is there any strong correlation between the features?\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "zKBaokFB3rR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.5 Model Building\n",
        "\n",
        "### 1.5.1 Linear Regression\n",
        "\n",
        "- `df.column` lists down the names of all the columns of the dataframe `df`.\n",
        "- `train_test_split()` helps to split the data into training and test datsets.\n",
        "- `StandardScaler` from `sklearn.preprocessing` helps to scale the data before modelling.\n",
        "- `OLS()` from `sm` (`statsmodels` library) helps to fit a linear regression model on the data. This is done as `model = sm.OLS(y_train, X_train).fit()`.\n",
        "- `model.summary()` provides a detailed summary with all the metrics of the fitted linear regression model."
      ],
      "metadata": {
        "id": "YZsPFea_fJju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the columns of the data to identify the features\n",
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wa2TfFTfRyP",
        "outputId": "b44c9aef-b27b-40bd-b00f-e603b370fd46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Ratings', 'RAM', 'ROM', 'Mobile_Size', 'Primary_Cam', 'Selfi_Cam',\n",
              "       'Battery_Power', 'Price'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select features and target variable for regression\n",
        "# Replace the placeholder with the names of all the columns in the dataset except the target variable - 'Price'\n",
        "features = <>\n",
        "\n",
        "# Split the data into features (X) and target (y)\n",
        "# Replace the placeholder <1> with the dataset containing the features only - say `df[features]`\n",
        "# Replace the placeholder <2> with the target column say `df['Price']`\n",
        "X = <1>\n",
        "y = <2>\n",
        "\n",
        "# Split into training and testing sets\n",
        "# Replace the placeholder <method to do split> with the methos used to do the test-train split - say `train_test_split`\n",
        "# Replace the placeholder <features dataset> with the variable where the dataset containing only the features are stored- say `X`\n",
        "# Replace the placeholder <target> with the variable where the target data is stored- say `y`\n",
        "X_train, X_test, y_train, y_test = <method to do split>(<features dataset>, <target>, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check the data\n",
        "X_train[:5]"
      ],
      "metadata": {
        "id": "Bh8OcIJDgE4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize the features\n",
        "# Import the library\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize a scaler\n",
        "# Replace the placeholder <> with the method to initiate the standard scaler - `StandardScaler()`\n",
        "scaler = <>\n",
        "\n",
        "# Scale the train and test data\n",
        "# Replace the placeholder <1> to fit & transform the training data using `scaler` - `scaler.fit_transform(X_train)`\n",
        "# Replace the placeholder <2> to transform the test data using `scaler` - `scaler.transform(X_test)`\n",
        "X_train = <1>\n",
        "X_test = <2>\n",
        "\n",
        "# Check the data\n",
        "X_train[:5]"
      ],
      "metadata": {
        "id": "8D6sDy_5gm5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Building using statsmodels library\n",
        "# Import the modules\n",
        "from statsmodels.formula.api import ols\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Add the intercept term\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# Splitting the data in 70:30 ratio of train to test data\n",
        "# Replace the placeholder <> with the code to split the test-train data in ration of 70:30 - `train_test_split(X, y, test_size = 0.30 , random_state = 1)`\n",
        "X_train, X_test, y_train, y_test = <>\n",
        "\n",
        "\n",
        "# Create the model\n",
        "# Replace the placeholder <> with code to fit the OLS model - `sm.OLS(y_train, X_train).fit()`\n",
        "# Note that `y` comes after `X` here in this\n",
        "model1 = <>\n",
        "\n",
        "# Get the model summary\n",
        "# Replace the placeholder <> with the code to provide the model summary - `model1.sumamry()`\n",
        "<>"
      ],
      "metadata": {
        "id": "1dgyapj-hZwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpretation:\n",
        "- What do you think about the metrics?\n",
        "  \n",
        "- Does the data suffers from multicollinearity?\n",
        "  "
      ],
      "metadata": {
        "id": "kPDLiMWyrkAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5.2 Multicollinearity Analysis\n",
        "Checking for multicollinearity involves assessing the degree to which independent variables in a regression model are highly correlated with each other.\n",
        "\n",
        "- **Variance Inflation Factor:**\n",
        "\n",
        "  - VIF, or Variance Inflation Factor, is a measure used to detect the presence of multicollinearity in a regression model.\n",
        "\n",
        "  - High VIF values indicate that a predictor variable is highly correlated with other predictors, which can make the coefficient estimates unstable and difficult to interpret.\n",
        "  - Low VIF values suggest little to no multicollinearity.\n",
        "\n",
        "- **Interpreting VIF values:**\n",
        "  - **VIF = 1:** No multicollinearity; the predictor is not correlated with other variables.\n",
        "  - **1 < VIF â‰¤ 5:** Low to moderate multicollinearity; generally acceptable but worth monitoring.\n",
        "  - **VIF > 5:** Indicates moderate multicollinearity; may start affecting the reliability of the model's coefficients.\n",
        "  - **VIF > 10:** High multicollinearity; problematic and often signals that the variable should be removed or combined with others.\n",
        "\n",
        "  *Note: Understand this code and run this. Your task for this section is to understand what is vif and how to interpret the values.*"
      ],
      "metadata": {
        "id": "mD9lPA2d3rR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the required modules\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# Assuming df is your DataFrame and it is already loaded\n",
        "# Remove non-numeric columns if any\n",
        "df_numeric = df.select_dtypes(include=[float, int])\n",
        "\n",
        "# Define a function to calculate VIF for each feature\n",
        "def calculate_vif(df):\n",
        "    vif_data = pd.DataFrame()\n",
        "    vif_data[\"Feature\"] = df.columns\n",
        "    vif_data[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(len(df.columns))]\n",
        "    return vif_data\n",
        "\n",
        "# Calculate VIF for the dataset\n",
        "vif_df = calculate_vif(df_numeric)\n",
        "\n",
        "# Display the VIF values\n",
        "print(vif_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35bb68c7-cc53-46af-eb4e-0da83993689a",
        "id": "XEiShgYn3rR_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Feature        VIF\n",
            "0        Ratings  41.563787\n",
            "1            RAM  11.955536\n",
            "2            ROM   3.684761\n",
            "3    Mobile_Size   3.067169\n",
            "4    Primary_Cam  21.168871\n",
            "5      Selfi_Cam   5.024410\n",
            "6  Battery_Power  18.591624\n",
            "7          Price   2.157319\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpretation:\n",
        "\n",
        "- Which features exhibit high multicollinearity?\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "- What action should be considered for features with high VIF values?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "y2dz3W-h3rR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5.3 Random Forest Regressor\n",
        "\n",
        "The Random Forest Regressor is an advanced machine learning algorithm that combines multiple classifiers(decision trees) to improve predictive accuracy and control overfitting.  The final prediction is obtained by averaging the predictions from all the individual classifiers. This approach reduces the variance of the model and is particularly effective in handling large datasets and capturing complex relationships between variables. Random Forest Regressors are also robust to multicollinearity and can provide insights into feature importance.\n",
        "\n",
        "*Note: You will learn about decision trees and random forest working in the later sessions through this course.*\n",
        "\n",
        "- `RandomForestRegressor` from `sklearn.ensemble` leps to fit a random forest regressor."
      ],
      "metadata": {
        "id": "ZOnnCEVGrVFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the module\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Initialize the Random Forest Regressor\n",
        "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "# Replace the placeholder with the code to fit the model using the model created - `rf_regressor.fit(X_train, y_train)`\n",
        "<>\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = rf_regressor.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Linear Regression Model Evaluation:\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "print(f\"R-squared (R2): {r2}\")"
      ],
      "metadata": {
        "id": "2mbPNXWYqQOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpretation:\n",
        "- Does the model perform well?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nmKBluMduz8T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5.4 Visualizing the result\n",
        "\n",
        "- `sns.scatterplot()` helps to plot the true values vs predicted values.\n",
        "- `plot()` helps to draw the diagonal line showing the fitted regression line."
      ],
      "metadata": {
        "id": "2VgHhQOExTki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a scatter plot of true values (y_test) vs. predicted values (y_pred)\n",
        "# Replace the placeholder <1> with the name of the method for visualizing - `sns.scatterplot()`\n",
        "# Replace the placeholder <2> with the variable containing the true y values of the test data - `y_test`\n",
        "# Replace the placeholder <3> with the variable containing the predicted y values of the test data - `y_pred`\n",
        "<1>(x=<2>, y=<3>, color='blue', s=50)\n",
        "\n",
        "# Add a diagonal line (red) that represents the ideal scenario where predicted values match the true values\n",
        "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', lw=2)  # Diagonal line\n",
        "\n",
        "# Label the x-axis as 'True Values'\n",
        "plt.xlabel('True Values')\n",
        "\n",
        "# Label the y-axis as 'Predicted Values'\n",
        "plt.ylabel('Predicted Values')\n",
        "\n",
        "# Set the title of the plot to 'True vs Predicted Values'\n",
        "plt.title('True vs Predicted Values')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mirxjI3m3rSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpretation:\n",
        "- What does the plot indicate about the model predictions?\n",
        "  \n",
        "  \n",
        "<hr> <hr>"
      ],
      "metadata": {
        "id": "GEqGBZfGxrjP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Classification Models\n"
      ],
      "metadata": {
        "id": "jEYrnUgVaPJu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Dataset Description\n",
        "\n",
        "Bob has started his own mobile company. He wants to give tough fight to big companies like Apple,Samsung etc.\n",
        "\n",
        "He does not know how to estimate price of mobiles his company creates. In this competitive mobile phone market you cannot simply assume things. To solve this problem he collects sales data of mobile phones of various companies.\n",
        "\n",
        "Bob wants to find out some relation between features of a mobile phone(eg:- RAM,Internal Memory etc) and its selling price. But he is not so good at Machine Learning. So he needs your help to solve this problem.\n",
        "\n",
        "In this problem you do not have to predict actual price but a price range indicating how high the price is.\n",
        "\n",
        "<center><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*D7TDOkHIH8hmbYQtCZ856A.jpeg\" width=500/></center>\n",
        "\n",
        "Dataset as 21 features and 2000 entries. The meanings of the features are given below.\n",
        "\n",
        "- `battery_power`: Total energy a battery can store in one time measured in mAh\n",
        "- `blue`: Has bluetooth or not\n",
        "- `clock_speed`: speed at which microprocessor executes instructions\n",
        "- `dual_sim`: Has dual sim support or not\n",
        "- `fc`: Front Camera mega pixels\n",
        "- `four_g`: Has 4G or not\n",
        "- `int_memory`: Internal Memory in Gigabytes\n",
        "- `m_dep`: Mobile Depth in cm\n",
        "- `mobile_wt`: Weight of mobile phone\n",
        "- `n_cores`: Number of cores of processor\n",
        "- `pc`: Primary Camera mega pixels\n",
        "- `px_height`: Pixel Resolution Height\n",
        "- `px_width`: Pixel Resolution Width\n",
        "- `ram`: Random Access Memory in Mega Byte\n",
        "- `sc_h`: Screen Height of mobile in cm\n",
        "- `sc_w`: Screen Width of mobile in cm\n",
        "- `talk_time`: longest time that a single battery charge will last when you are\n",
        "- `three_g`: Has 3G or not\n",
        "- `touch_screen`: Has touch screen or not\n",
        "- `wifi`: Has wifi or not\n",
        "-`price_range`: This is the target variable with value of 0(low cost), 1(medium cost), 2(high cost) and 3(very high cost)\n"
      ],
      "metadata": {
        "id": "oD2FQFOMcVHV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Importing the required libraries & modules"
      ],
      "metadata": {
        "id": "UnKTzuUy5hsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the warnings module, which allows control over warning messages\n",
        "import warnings\n",
        "\n",
        "# Suppress all warnings in the script\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "QKpJtaepg2Lk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries for numerical operations and data manipulation\n",
        "import numpy as np     # For numerical computations and array manipulations\n",
        "import pandas as pd    # For handling DataFrames, and data loading, cleaning, and manipulation\n",
        "\n",
        "# Importing libraries for data visualization\n",
        "import matplotlib.pyplot as plt  # For creating basic plots (histograms, scatter plots, etc.)\n",
        "import seaborn as sns            # For enhanced and more aesthetically pleasing visualizations\n",
        "\n",
        "# Importing machine learning models and evaluation metrics\n",
        "from sklearn.naive_bayes import GaussianNB                      # Gaussian Naive Bayes classifier\n",
        "from sklearn.neighbors import KNeighborsClassifier              # K-Nearest Neighbors (KNN) classifier\n",
        "from sklearn.linear_model import LogisticRegression             # Logistic Regression classifier\n",
        "\n",
        "# Importing utilities for model evaluation and hyperparameter tuning\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV  # For splitting the dataset into training and test sets, and grid search for hyperparameter tuning\n",
        "\n",
        "# Importing evaluation metrics for classification tasks\n",
        "from sklearn.metrics import (accuracy_score, classification_report,  # Metrics to evaluate model performance\n",
        "                             precision_score, recall_score,\n",
        "                             f1_score, confusion_matrix)\n"
      ],
      "metadata": {
        "id": "jsfn-GQhV-iQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Loading & Viewing the data\n",
        "\n",
        "- `read_csv()` helps to load a csv file.\n",
        "- `head()` helps to view the initial few rows of the dataset.\n",
        "- `df.shape` gives the shape (rows by columns) of the dataset `df`."
      ],
      "metadata": {
        "id": "3tglI_1r5tiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "# Replace the placeholder to load the data - `pd.read_csv('mobile_data_new.csv')`\n",
        "df = <>\n",
        "\n",
        "# View the data\n",
        "# Replace the placeholder with the code to view the first few rows of the data\n",
        "<>"
      ],
      "metadata": {
        "id": "zoZ7CWFAcO7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the shape of the DataFrame\n",
        "df.shape"
      ],
      "metadata": {
        "id": "m0MCkxTK6EVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 Exploratory Data Analysis (EDA)\n"
      ],
      "metadata": {
        "id": "ylUn6Aw7eSXY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4.1 Basic Information\n",
        "\n",
        "- `info()` helps to get the basic information about the dataset.\n",
        "- `value_counts()` count the number of occurrences of each unique value in a DataFrame column.\n"
      ],
      "metadata": {
        "id": "ScvgSOPuS9TQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the basic info of the DataFrame\n",
        "df.info()"
      ],
      "metadata": {
        "id": "mU-craoCM0Rz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpretation:\n",
        "- Are there any null values in the dataset?\n",
        "  \n"
      ],
      "metadata": {
        "id": "2gXg5HeHCwGE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4.2 Examine the Target Variable\n"
      ],
      "metadata": {
        "id": "s5AoVI3UTP3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the target variable distribution\n",
        "# Replace the placeholder <> with the method used to find the unique values in a column - `value_counts()`\n",
        "df['price_range'].<>(normalize=True)\n"
      ],
      "metadata": {
        "id": "BePL0FgBF11q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpretation:\n",
        "\n",
        "- Does the dataset have class imbalance?\n"
      ],
      "metadata": {
        "id": "EdN9S16hUdr9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4.3 Correlation Analysis\n",
        "\n",
        "*Hint: Use heatmap*\n",
        "- `corr()` calculates the correlation between columns in a DataFrame.\n",
        "- `sns.heatmap()` helps to visualize the correlation values as a color coded matrix."
      ],
      "metadata": {
        "id": "7ra-1nggIgzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the correlation matrix for the DataFrame to understand the relationships between numerical variables\n",
        "# Replace the placeholder with the code to find the correlation values of the dataset - `df.corr()`\n",
        "corr = <>\n",
        "\n",
        "# Set up the figure for the heatmap with a size of 10x8 inches\n",
        "plt.figure(figsize=(18, 10))\n",
        "\n",
        "# Plot the heatmap of the correlation matrix\n",
        "# annot=True adds the correlation coefficients on the heatmap\n",
        "# cmap='Blues' specifies the color map\n",
        "# fmt='.2f' formats the annotation to two decimal places\n",
        "\n",
        "# Replace the placeholder with the variable having the correlation values\n",
        "sns.heatmap(<>, annot=True, cmap='Blues', fmt='.2f')\n",
        "\n",
        "# Add a title to the heatmap\n",
        "plt.title('Correlation Heatmap')\n",
        "\n",
        "# Display the heatmap\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yMKD6S07idNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpretation:\n",
        "\n",
        "- Which features are most strongly correlated with the target variable `price_range`?\n",
        "   \n",
        "\n",
        "- Are there any features that have a notable correlation with each other?\n",
        "  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3s-qvWbYjhfa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 Feature Engineering\n",
        "\n",
        "### 2.5.1 Create new features based on highly correlated ones to avoid multicollinearity\n",
        "\n",
        "This code performs feature engineering on a DataFrame `df` by creating new variables that capture more meaningful relationships between the original features and then drops the original features that were used to create these new variables.\n",
        "\n",
        "1. **Total Pixel Resolution (`px_total`)**:  \n",
        "   - Calculates the total pixel resolution by multiplying the pixel height (`px_height`) and pixel width (`px_width`), capturing the overall screen resolution.\n",
        "\n",
        "2. **Camera Quality Score (`cam_quality`)**:  \n",
        "   - Sums the primary camera (`pc`) and front camera (`fc`) megapixels to create a composite score that represents the overall camera quality of the device.\n",
        "\n",
        "3. **Mobile Volume (`mobile_volume`)**:  \n",
        "   - Computes the mobile's physical volume by multiplying screen height (`sc_h`), screen width (`sc_w`), and mobile depth (`m_dep`), representing the size or form factor of the device.\n",
        "\n",
        "4. **Screen Area (`screen_area`)**:  \n",
        "   - Calculates the screen area by multiplying the screen height (`sc_h`) and screen width (`sc_w`), representing the display size of the device.\n",
        "\n",
        "5. **Dropping Original Features**:  \n",
        "   - Removes the original features (`px_height`, `px_width`, `pc`, `fc`, `sc_h`, `sc_w`, `m_dep`) that were used to create the new variables, keeping the DataFrame cleaner and focused on the engineered features.\n",
        "\n",
        "*Note: Your task in this section is to understand how new variables are created as a part of feature engineering technique.*\n"
      ],
      "metadata": {
        "id": "FXOHpm49h5Id"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Total Pixel Resolution (`px_total`)\n",
        "df['px_total'] = df['px_height'] * df['px_width']\n",
        "\n",
        "# 2. Camera Quality Score (`cam_quality`)\n",
        "df['cam_quality'] = df['pc'] + df['fc']\n",
        "\n",
        "# 3. Mobile Volume (`mobile_volume`)\n",
        "df['mobile_volume'] = df['sc_h'] * df['sc_w'] * df['m_dep']\n",
        "\n",
        "# 7. Screen Aspect Ratio (`screen_aspect_ratio`)\n",
        "df['screen_area'] = df['sc_h'] * df['sc_w']\n",
        "\n",
        "# Dropping the original features\n",
        "# Replace the placeholder with the list of columns to be dropped. - ['px_height', 'px_width', 'pc', 'fc', 'sc_h', 'sc_w', 'm_dep']\n",
        "df = df.drop(columns=<>)\n",
        "\n",
        "# Display the first few rows to check the new features\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "m7_6Sc0G9JQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate the features (X) and the target variable (y) for the  data\n",
        "\n",
        "# Replace the placeholder <1> with code to extract features - `df.drop(['price_range'], axis = 1)`\n",
        "# Replace the placeholder <2> with the target column - `df[['price_range']]`\n",
        "X, y = <1>, <2>\n"
      ],
      "metadata": {
        "id": "tfSIVFwzhmt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.6 Model Building\n",
        "### 2.6.1 Spliting Train and Test data\n",
        "\n",
        "- `train_test_split()` helps to split the data into training and test datsets.\n"
      ],
      "metadata": {
        "id": "pewcFI0wAt47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "# Replace the placeholder with the doe to do the test-train split - `X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42)`\n",
        "<>\n",
        "# Check the data\n",
        "X_train[:5]"
      ],
      "metadata": {
        "id": "xl6wvBlykaRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6.2 Feature Scaling\n",
        "\n",
        "- `StandardScaler` from `sklearn.preprocessing` helps to scale the data before modelling.\n"
      ],
      "metadata": {
        "id": "TR6U6Jq7gCMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the library\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize the StandardScaler, which will standardize features by removing the mean and scaling to unit variance\n",
        "scaler = StandardScaler()\n",
        "\n",
        "\n",
        "# Fit the scaler on the training data and apply the transformation (standardization) to X_train\n",
        "# Replace the placeholder <> with the code to fit & transform the train data - `X_train = scaler.fit_transform(X_train)`\n",
        "<>\n",
        "# Apply the same transformation to the test data (X_test) using the already fitted scaler\n",
        "# Replace the placeholder <> to apply the same transformation to test data - `X_test = scaler.transform(X_test)`\n",
        "<>"
      ],
      "metadata": {
        "id": "NEfnQrpnf_nV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.7 Model Building\n",
        "\n",
        "Make sure the following imports are done for model building.\n",
        "```\n",
        "from sklearn.naive_bayes import GaussianNB  \n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier  \n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "```\n",
        "\n",
        "- `classification_report()` from `sklearn.metrics` helps to view the model metrics.\n",
        "\n",
        "           \n",
        "\n",
        "\n",
        "### 2.7.1 KNN Algorithm"
      ],
      "metadata": {
        "id": "baYBe4HhyWkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the KNN model\n",
        "# Replace the placeholder <> with the KNN classifier - `KNeighborsClassifier()`\n",
        "model = <>\n",
        "\n",
        "# Fit the model\n",
        "# Replace the placeholder with the code to fit the model - `model.fit(X_train, y_train)`\n",
        "<>\n",
        "\n",
        "# Make predictions\n",
        "# Replace the placeholder <> with the code to make predictions in the test data - `model.predict(X_test)`\n",
        "y_pred = <>\n",
        "\n",
        "# Calculate accuracy\n",
        "# Replace the placeholders <1>, <2> with the true 'y' values and predicted 'y' values respectively\n",
        "accuracy = accuracy_score(<1>, <2>)\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# Print classification report\n",
        "# Replace the placeholders <1>, <2> with the true 'y' values and predicted 'y' values respectively\n",
        "print('Classification Report:')\n",
        "print(classification_report(<1>, <2>))"
      ],
      "metadata": {
        "id": "t0LEO20uNoO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpretation:\n",
        "- Which class does the model perform best and worst on?\n",
        "  \n",
        "- Do you think the value of `k` has impacted the performance?\n",
        "  \n",
        "*Note: Your task is to run the code and understand how 'k' values influence the model metrics.*"
      ],
      "metadata": {
        "id": "kAJv0-czO08m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rebuild the model with 'k' value as 10 and 25\n",
        "\n",
        "# Initialize the KNN models\n",
        "model1 = KNeighborsClassifier(n_neighbors=10)\n",
        "model2 = KNeighborsClassifier(n_neighbors=25)\n",
        "\n",
        "# Fit the models\n",
        "model1.fit(X_train, y_train)\n",
        "model2.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred1 = model1.predict(X_test)\n",
        "y_pred2 = model2.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy1 = accuracy_score(y_test, y_pred1)\n",
        "accuracy2 = accuracy_score(y_test, y_pred2)\n",
        "print(f'Accuracy for model with k=10: {accuracy1:.4f}')\n",
        "print(f'Accuracy for model with k=25: {accuracy2:.4f}')\n",
        "\n",
        "# Print classification report\n",
        "print('Classification Report for model with k=10:')\n",
        "print(classification_report(y_test, y_pred1))\n",
        "\n",
        "print('Classification Report for model with k=25:')\n",
        "print(classification_report(y_test, y_pred2))"
      ],
      "metadata": {
        "id": "qKdoeCXUPjei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpretation:\n",
        "- Which model performed better overall, and what evidence supports this?\n",
        "  \n",
        "\n",
        "- Should even larger `k` values be considered for further improvement?\n",
        "  "
      ],
      "metadata": {
        "id": "L09qoWwLQ61q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.7.2 Naive Bayes Algorithm\n",
        "\n",
        "Let's try Gaussian Naive Bayes algorithm.\n"
      ],
      "metadata": {
        "id": "DjVqoLsOzJZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Gaussian Naive Bayes model\n",
        "# Replace the placeholder with the Naive Bayes classifier - `GaussianNB()`\n",
        "gnb = <>\n",
        "\n",
        "# Fit the model on the training data\n",
        "# Replace the placeholders with the parameters to fit the model - `X_train, y_train`\n",
        "gnb.fit(<>)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# Print classification report\n",
        "print('Classification Report:')\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "xDTWkXUP06aO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpretation:\n",
        "- Does the Gaussian Naive Bayes model performing better compared to the KNN models? Why?\n",
        "  \n",
        "\n",
        "- Does this model generalize well for all the classes?\n",
        "  "
      ],
      "metadata": {
        "id": "PXgS4RRxRwJ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.7.3 Logistic Regression"
      ],
      "metadata": {
        "id": "Mfjo6NpaOP1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Logistic Regression model\n",
        "# Replace the place holder <> with the logistic regression model - `LogisticRegression(random_state=42)`\n",
        "log_reg = <>\n",
        "\n",
        "# Train the model using the training data\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "\n",
        "# Calculate accuracy\n",
        "# Replace the placeholder <1> to calculate accuracy - `accuracy_score(y_test, y_pred)`\n",
        "accuracy = <1>\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# Print classification report\n",
        "# Replace the placeholder <2> to print the classification report - `classification_report(y_test, y_pred)`\n",
        "print('Classification Report:')\n",
        "print(<2>)\n"
      ],
      "metadata": {
        "id": "_i5cShT4NvHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpretation:\n",
        "- How well did the Logistic Regression model perform overall?\n",
        "  \n",
        "\n",
        "- How does the model perform across different classes in terms of precision, recall, and f1-score?\n",
        "\n",
        "  \n",
        "\n",
        "- Why might Logistic Regression be performing better than the previous models?\n",
        "  "
      ],
      "metadata": {
        "id": "FsRVIiwtSjTv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.7.3.1 Finding the top 10 features contributing for the classification\n",
        "\n",
        "\n",
        "*Note: Run the code and understand which features contribute the most.*"
      ],
      "metadata": {
        "id": "13XUCZ87TclL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the coefficients from the model\n",
        "coefficients = log_reg.coef_[0]  # For binary classification, for multiclass use model.coef_\n",
        "\n",
        "# Create a DataFrame to associate feature names with their coefficients\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Coefficient': coefficients\n",
        "})\n",
        "\n",
        "# Take the absolute value of coefficients to sort by importance\n",
        "feature_importance['Absolute Coefficient'] = feature_importance['Coefficient'].abs()\n",
        "\n",
        "# Sort the features by the absolute value of their coefficients\n",
        "top_features = feature_importance.sort_values(by='Absolute Coefficient', ascending=False)\n",
        "\n",
        "# Display the top features\n",
        "print(top_features.head(10))  # Display top 10 features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ka4YrLNjTOth",
        "outputId": "adcc9708-c063-4e3b-bf78-b3794a31ef4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          Feature  Coefficient  Absolute Coefficient\n",
            "8             ram   -10.503915             10.503915\n",
            "0   battery_power    -2.625744              2.625744\n",
            "13       px_total    -2.516479              2.516479\n",
            "6       mobile_wt     0.369839              0.369839\n",
            "14    cam_quality    -0.143826              0.143826\n",
            "3        dual_sim     0.132239              0.132239\n",
            "10        three_g    -0.120281              0.120281\n",
            "2     clock_speed     0.098988              0.098988\n",
            "5      int_memory    -0.093072              0.093072\n",
            "4          four_g     0.081853              0.081853\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpretation:\n",
        "- What is the top feature affecting the target variable?\n",
        "  \n",
        "- Does this contain any newly created feature variables?\n",
        "  \n",
        "\n",
        "<hr> <hr>"
      ],
      "metadata": {
        "id": "uzlC21kDTsg-"
      }
    }
  ]
}